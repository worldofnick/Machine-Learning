\documentclass[11pt]{article}
\usepackage{euscript}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{xspace}
\usepackage{color}
\usepackage{url}
\usepackage{subfig}
\usepackage{float}
\usepackage{array}
\graphicspath{ {images/} }
%%%%%%%  For drawing trees  %%%%%%%%%
\usepackage{tikz}
\usetikzlibrary{calc, shapes, backgrounds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\textheight}{9in}
\setlength{\topmargin}{-0.600in}
\setlength{\headheight}{0.2in}
\setlength{\headsep}{0.250in}
\setlength{\footskip}{0.5in}
\flushbottom
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\columnsep}{2pc}
\setlength{\parindent}{1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\eps}{\varepsilon}

\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\s}[1]{\textsf{#1}}
\newcommand{\tb}[1]{\textbf{#1}}

\newcommand{\E}{\textbf{\textsf{E}}}
\renewcommand{\Pr}{\textbf{\textsf{Pr}}}
\newcommand*{\escape}[1]{\texttt{\textbackslash#1}}


\title{\textbf{\underline{Perceptron Learning Algorithm Notes}}}


\author{Nick R. Porter}

\begin{document}
\maketitle

\section{Perceptron Overview}

\begin{itemize}
	\item Binary classifier, can only be used to predict -1 or 1.
	\item If the data is not linearly separable then the algorithm will never terminate unless a max number of iterations is set.
	\item The algorithm takes a vector $x$ for each feature of the data.
	\item We also learn a vector $w$ of weights each weight corresponds to a feature in $x$. Sometimes have a $w_0$ which is a constant/offset.
	\item Returns a value (-1, 1) that corresponds to a class of the data.
	\item We can define a function $g(x,w) = w_0x_0 + w_1x_1 + ... + w_mx_m = w \cdot x$
	\item If $g(x,w)$ returns a value $0$ or greater(or some threshold $theta$) return $1$. Otherwise return $-1$
	\item Leads us to define an activation function $\phi(g(x,w)) = 1$ if $g(x,w) \geq 1$, else $-1$
	\item We learn these weights from training data
	
\end{itemize}

\section{Training}

\begin{itemize}
\item Algorithm outline:
	\begin{itemize}
	\item Initialize all weights to $0$ or small random numbers.
	\item For each training sample $x^{(i)}$ perform the following setps:

		\begin{itemize}
		\item Compute the predicted value, $\hat y$
		\item Update weights
		\item Update to each weight is defined as $w_j := w_j + \Delta w_j$
		\item $\Delta w$ is computed using the perceptron learning rule.
		\item $\Delta w_j = \alpha(y^{(i)} - \hat y^{(i)})x^{(i)}_{j}$
		\end{itemize}
		
		\item Here alpha is the learning rate, usually a small number around $0.01$.
		\item We see that if the prediction is correct then $\Delta w_j = 0$

	\end{itemize}	
\end{itemize}
	
\end{document}
